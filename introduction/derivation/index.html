<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.110.0"><meta name=generator content="Relearn 5.10.2+tip"><meta name=description content="Introduction and educational site for the design and optimization of parallel domain flow algorithms"><meta name=author content="E. Theodore L. Omtzigt"><title>Derivation of the matrix multiply domain flow program - Domain Flow Architecture</title><link href=../../images/favicon.png?1675444628 rel=icon type=image/png><link href=../../css/fontawesome-all.min.css?1675444628 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=../../css/fontawesome-all.min.css?1675444628 rel=stylesheet></noscript><link href=../../css/featherlight.min.css?1675444628 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=../../css/featherlight.min.css?1675444628 rel=stylesheet></noscript><link href=../../css/auto-complete.css?1675444628 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=../../css/auto-complete.css?1675444628 rel=stylesheet></noscript><link href=../../css/perfect-scrollbar.min.css?1675444628 rel=stylesheet><link href=../../css/nucleus.css?1675444628 rel=stylesheet><link href=../../css/fonts.css?1675444628 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=../../css/fonts.css?1675444628 rel=stylesheet></noscript><link href=../../css/theme.css?1675444628 rel=stylesheet><link href=../../css/theme-blue.css?1675444628 rel=stylesheet id=variant-style><link href=../../css/ie.css?1675444628 rel=stylesheet><link href=../../css/variant.css?1675444628 rel=stylesheet><link href=../../css/print.css?1675444628 rel=stylesheet media=print><script src=../../js/url.js?1675444628></script>
<script src=../../js/variant.js?1675444628></script>
<script>window.index_json_url="../../index.json";var root_url="../../",baseUriFull,baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="Copy to clipboard",window.T_Copied_to_clipboard="Copied to clipboard!",window.T_Copy_link_to_clipboard="Copy link to clipboard",window.T_Link_copied_to_clipboard="Copied link to clipboard!",window.T_No_results_found="No results found for \u0022{0}\u0022",window.T_N_results_found="{1} results found for \u0022{0}\u0022",baseUriFull="https://stillwater-sc.github.io/domain-flow/",window.variants&&variants.init(["blue","green","red","relearn-light","relearn-dark"])</script><script src=../../js/jquery.min.js?1675444628 defer></script></head><body class="mobile-support html" data-url=../../introduction/derivation/index.html><div id=body class=default-animation><div id=sidebar-overlay></div><div id=toc-overlay></div><nav id=topbar class=highlightable dir=ltr><div><div class=navigation><a class="nav nav-next" href=../../introduction/wavefront/index.html title="Wavefronts of Computation (&#129106;)"><i class="fas fa-chevron-right fa-fw"></i></a></div><div class=navigation><a class="nav nav-prev" href=../../introduction/linearschedule/index.html title="Linear Schedules (&#129104;)"><i class="fas fa-chevron-left fa-fw"></i></a></div><div id=breadcrumbs><span id=sidebar-toggle-span><a href=# id=sidebar-toggle title='Menu (CTRL+ALT+n)'><i class="fas fa-bars fa-fw"></i></a></span><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=../../index.html><span itemprop=name>Domain Flow Architecture</span></a><meta itemprop=position content="1">></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=../../introduction/index.html><span itemprop=name>Getting Started</span></a><meta itemprop=position content="2">></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Derivation of the matrix multiply domain flow program</span><meta itemprop=position content="3"></li></ol></div></div></nav><main id=body-inner class="highlightable default" tabindex=-1><div class=flex-block-wrapper><div id=head-tags><div class=tags><a class=tag-link href=../../tags/derivation/index.html>derivation</a>
<a class=tag-link href=../../tags/domain-flow/index.html>domain-flow</a>
<a class=tag-link href=../../tags/matrix-multiply/index.html>matrix-multiply</a></div></div><article class=default><h1 id=derivation-of-the-matrix-multiply-domain-flow-program>Derivation of the matrix multiply domain flow program</h1><h1 id=heading></h1><p>The concepts of partial and total orders are essential for finding optimal domain flow algorithms.
Partial orders, or <a href=https://en.wikipedia.org/wiki/Partially_ordered_set target=_blank>Poset</a>, are the
source of high-performance, low-power execution patterns.</p><p>The Linear Algebra universe is particularly rich in partial orders, something that has been exploited
for centuries <sup><a href=#history>1</a></sup>. Matrix Computations<sup><a href=#matrix-computations>2</a></sup> by Golub, and van Loan provide
a comprehensive review. What follows may be a bit technical, but keep in mind the visualizations of the previous
pages as you try to visualize what the math implies.</p><p>We want to evaluate the matrix-matrix multiplication:
<span class="math align-center">$ C = A \otimes B $</span>, where
<span class="math align-center">$A$</span>,
<span class="math align-center">$B$</span>, and
<span class="math align-center">$C$</span>
are matrices of size
<span class="math align-center">$N \times N$</span>.
We picked the square matrix version because it is cleaner to visualize the symmetry in the computational
wavefront, but all that will follow will work just as well when the matrices are rectangular.</p><p>Linear algebra operators exhibits many independent operations. For example, there are four basic vector operators:</p><ol><li>scale</li><li>add</li><li>multiply, and</li><li>dot product</li></ol><p>The operator
<span class="math align-center">$z = alpha * x + y$</span> is frequently used, and although redundant, tends
to be added as the fifth operator, and referred to as the <em>saxpy</em> operator for &ldquo;Scalar Alpha X Plus Y&rdquo;.
The <em>dot product</em> is also referred to as the <em>inner product</em>. The <em>inner product</em> is an operator that
brings two vectors together into a scalar representing a measure how much the vectors point in the same direction.
The <em>outer product</em> is an operator that expands two vectors into a matrix: for vector
<span class="math align-center">$x$</span> and
<span class="math align-center">$y$</span>, the outer product
<span class="math align-center">$\times$</span>
is defined as:
<span class="math align-center">$xy^T$</span>.</p><p>Matrix operations can be expressed in terms of these vector operators with many degrees of freedom.
For example, &lsquo;double loop&rsquo; matrix-vector multiplication can be arranged in
<span class="math align-center">$2! = 2$</span>
different ways; we can evaluate the inner products, or we can evaluate the outer products.</p><p>&lsquo;Triple loop&rsquo; matrix-matrix multiplication can be arranged in
<span class="math align-center">$3! = 6$</span> ways.
These arrangements have their own operators and their own modes of access, and thus the interplay
with a spatial distribution of the rows and columns of the matrices is key to
evaluate the efficiency of the computation. The orderings and properties of these different
arrangements is shown in Table 1:</p><table><thead><tr><th>Loop order</th><th>Inner Loop</th><th>Middle Loop</th><th>Inner Loop Data Access</th></tr></thead><tbody><tr><td>ijk</td><td>dot</td><td>vector
<span class="math align-center">$\times$</span> matrix</td><td><span class="math align-center">$A$</span> by row,
<span class="math align-center">$B$</span> by column</td></tr><tr><td>jik</td><td>dot</td><td>matrix
<span class="math align-center">$\times$</span> vector</td><td><span class="math align-center">$A$</span> by row,
<span class="math align-center">$B$</span> by column</td></tr><tr><td>ikj</td><td>saxpy</td><td>row saxpy</td><td><span class="math align-center">$B$</span> by row</td></tr><tr><td>jki</td><td>saxpy</td><td>column saxpy</td><td><span class="math align-center">$A$</span> by column</td></tr><tr><td>kij</td><td>saxpy</td><td>row outer product</td><td><span class="math align-center">$B$</span> by row</td></tr><tr><td>kji</td><td>saxpy</td><td>column outer product</td><td><span class="math align-center">$A$</span> by column</td></tr></tbody></table><p><em>Table 1:</em> Matrix Multiplication: Orderings and Properties (see <sup><a href=#matrix-computations>2</a></sup>)</p><p>The <em><em>scale</em></em>, <em><em>add</em></em>, and <em><em>multiply</em></em> operators are highly parallel in that each individual
vector element operation is independent of each other. The <em><em>dot</em></em> product adds a consolidation,
or <em>contraction</em> phase to yield a single, scalar valued result.</p><p>In a parallel context, all these vector operators have an information distribution phase that is non-trivial.
First, vectors must be embedded in space, and secondly, the vectors need to be aligned so that these
element operations can commence. Progressing to matrix operators, we have vectors of vectors that need
to be aligned. For the domain flow algorithm we demonstrated, we started from the matrix-multiply expression
as
<span class="math align-center">$N^2$</span> independent <em>dot</em> products.
In mathematical terms: if we partition the
<span class="math align-center">$A$</span> matrix in rows, and the
<span class="math align-center">$B$</span> matrix in columns:</p><span class="math align-center">$$A = \begin{bmatrix} a_1^T \\\\ a_2^T \\\\ \vdots \\\\ a_n^T \end{bmatrix}$$</span><p>and</p><span class="math align-center">$$B = \begin{bmatrix} b_1 & b_2 & \cdots & b_n \end{bmatrix}$$</span><p>then matrix multiplication can be expressed as the collection of <em>dot</em> products:</p><span class="math align-center">$$\text{for i = 1:N} \\\\ \qquad \text{for j = 1:N} \\\\ \qquad\qquad c_{ij} = a_i^T b_j $$</span><p>Looking just at the individual <em>dot</em> product, a theoretical computer scientist would say: the fastest way
to evaluate a <em>dot</em> product is through a binary tree of depth
<span class="math align-center">$log(N)$</span>
yielding the result in
<span class="math align-center">$log(N)$</span> steps. A spec is written and handed off to a
hardware engineer. When the hardware engineer looks at this problem, a very different view emerges.
In the hardware world, an algebraic operator such as multiply or add evaluates, depending on the number system,
in the order of <em>1 nsec</em>. But sending the result across even a modestly sized chip, say 10x10 mm,
can take 10 times as long. If the result needs to be communicated across the network, it can take
a 1,000,000 times longer. With modern chip technology, it takes about the same time to compute
a multiply or add as it does to communicate the result to a local neighborhood. From the perspective
of electrons participating in the evaluation of an algebraic operator, computation and communication
are roughly equal in terms of time and thus distance these electrons can &lsquo;drift&rsquo;.</p><p>What this means for evaluating the <em>dot</em> product is that the evaluation of
<span class="math align-center">$$c_{ij} = \sum\limits_{k=1}^N a_{ik} * b_{kj}$$</span>
can be executed efficiently as a propagation through local neighborhoods of communicating functional units.
In mathematical terms we can write this as a recurrence:
<span class="math align-center">$$c_{ijk} = c_{ijk-1} + a_{ik} * b_{kj}$$</span>
You can start to recognize the domain flow algorithm as presented in our example. However, if we
distribute the
<span class="math align-center">$c_{ij}$</span> propagation in that
<span class="math align-center">$k$</span>-dimension,
then accesses to
<span class="math align-center">$a_{ik}$</span> and
<span class="math align-center">$b_{kj}$</span> are not local at all.
Is there a mechanism to get the correct
<span class="math align-center">$a$</span> and
<span class="math align-center">$b$</span> elements
to the right location?</p><p>Let&rsquo;s take a look at the dot products for
<span class="math align-center">$c_{1,1}$</span>,
<span class="math align-center">$c_{1,2}$</span>,
and
<span class="math align-center">$c_{2,1}$</span>. Visualize the propagation of the
<span class="math align-center">$c$</span>
recurrence along the
<span class="math align-center">$k$</span>-dimension above the point
<span class="math align-center">$i = 1, j = 1, k = 0$</span>. Let&rsquo;s position the row of
<span class="math align-center">$A$</span>,
<span class="math align-center">$a_1^T$</span>, alongside the
<span class="math align-center">$c_{1,1}$</span> propagation in the
<span class="math align-center">$j = 0$</span> plane. Thus,
<span class="math align-center">$a_{1,1}$</span> is presented at the
lattice point
<span class="math align-center">$(1,0,1)$</span>, and
<span class="math align-center">$a_{1,N}$</span> is positioned
at the lattice point
<span class="math align-center">$(1,0,N)$</span>. Similarly, let&rsquo;s position the column of
<span class="math align-center">$B$</span>,
<span class="math align-center">$b_1$</span>, alongside the
<span class="math align-center">$c_{1,1}$</span> propagation,
but position it in the
<span class="math align-center">$i = 0$</span> plane. That would imply that
<span class="math align-center">$b_{1,1}$</span>
is presented at the lattice point
<span class="math align-center">$(0,1,1)$</span>, and
<span class="math align-center">$b_{N,1}$</span>
is positioned at the lattice point
<span class="math align-center">$(0,1,N)$</span>. This would transform the recurrence
equation for
<span class="math align-center">$c_{1,1}$</span> into:</p><span class="math align-center">$$c_{1,1,k} = c_{1,1,k-1} + a_{1,0,k} * b_{0,1,k}$$</span><p>This recurrence represents all local neighborhood operand communications.</p><p>If we now visualize the recurrence for
<span class="math align-center">$c_{1,2}$</span> to propagate in the
<span class="math align-center">$k$</span>-column above the lattice point
<span class="math align-center">$(1,2,0)$</span> we
recognize that for
<span class="math align-center">$c_{1,2}$</span> we need the same row vector of
<span class="math align-center">$A$</span>
as the recurrence for
<span class="math align-center">$c_{1,1}$</span>. We can thus propagate the elements of the row
vector
<span class="math align-center">$a_1^T$</span> along the
<span class="math align-center">$j$</span>-direction and build up one side
of the <em>dot</em> products for the row
<span class="math align-center">$c_1^T$</span>.</p><p>Generalized to the whole
<span class="math align-center">$A$</span> matrix, this is a set of propagation recurrences
defined by:
<span class="math align-center">$$ a_{i,j,k} = a_{i,j-1,k}$$</span></p><p>Similarly, the column vector
<span class="math align-center">$b_1^T$</span> is shared between the recurrence of
<span class="math align-center">$c_{1,1}$</span> and
<span class="math align-center">$c_{2,1}$</span>, and we can propagate
the elements of the column vector
<span class="math align-center">$b_1^T$</span> along the
<span class="math align-center">$i$</span>-direction
and build up the other side of the <em>dot</em> products for the column
<span class="math align-center">$c_1$</span>.
Generalized to the whole
<span class="math align-center">$B$</span> matrix, this is a set of propagation recurrences
defined by:
<span class="math align-center">$$ b_{i,j,k} = b_{i-1,j,k}$$</span></p><p>Once we have the
<span class="math align-center">$A$</span> and
<span class="math align-center">$B$</span> matrix elements distributed
throughout the lattice, we can finally transform the
<span class="math align-center">$c$</span> recurrence into a
local neighborhood operand communication as well:</p><span class="math align-center">$$c_{i,j,k} = c_{i,j,k-1} + a_{i,j-1,k} * b_{i-1,j,k}$$</span><p>This completes the transformation to all local neighborhood operand communications with the
system of recurrences we have seen before expressed as a domain flow program:</p><pre tabindex=0><code>
compute ( (i,j,k) | 1 &lt;= i,j,k &lt;= N ) {
    a: a[i,j-1,k]
    b: b[i-1,j,k]
    c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k]
}
    
</code></pre><p><a name=history>1</a>: <a href=https://mathshistory.st-andrews.ac.uk/HistTopics/Matrices_and_determinants/ target=_blank>History of Matrices and Determinants</a></p><p><a name=matrix-computations>2</a>: <a href=https://cs.cornell.edu/cv/cvl_home/books/ target=_blank>Matrix Computations, Gene Golub and Charles van Loan</a></p><footer class=footline></footer></article></div></main></div><aside id=sidebar class="default-animation showVisitedLinks" dir=ltr><div id=header-wrapper class=default-animation><div id=header class=default-animation><a href=https://stillwater-sc.github.io/domain-flow><img src=https://stillwater-sc.github.io/domain-flow/images/stillwater-logo.png alt="Stillwater Supercomputing, Inc."></a></div><form action=../../search.html method=get><div class="searchbox default-animation"><button type=submit title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
<label class=a11y-only for=search-by>Search</label>
<input data-search-input id=search-by name=search-by class=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div></form><script>var contentLangs=["en"]</script><script src=../../js/auto-complete.js?1675444628 defer></script>
<script src=../../js/lunr/lunr.min.js?1675444628 defer></script>
<script src=../../js/lunr/lunr.stemmer.support.min.js?1675444628 defer></script>
<script src=../../js/lunr/lunr.multi.min.js?1675444628 defer></script>
<script src=../../js/lunr/lunr.en.min.js?1675444628 defer></script>
<script src=../../js/search.js?1675444628 defer></script></div><div id=content-wrapper class=highlightable><ul class=topics><li data-nav-id=/introduction/index.html class="dd-item parent alwaysopen"><a href=../../introduction/index.html>Getting Started<i class="fas fa-check read-icon"></i></a><ul id=subsections-db5db1cf4b1800282ddcebf5cde99c36><li data-nav-id=/introduction/example/index.html class=dd-item><a href=../../introduction/example/index.html>An Example<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/parallel-programming/index.html class=dd-item><a href=../../introduction/parallel-programming/index.html>Parallel Programming<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/spacetime/index.html class=dd-item><a href=../../introduction/spacetime/index.html>Spacetime<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/computational-spacetime/index.html class=dd-item><a href=../../introduction/computational-spacetime/index.html>Computational Spacetime<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/domain-flow/index.html class=dd-item><a href=../../introduction/domain-flow/index.html>Domain Flow<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/freeschedule/index.html class=dd-item><a href=../../introduction/freeschedule/index.html>Free Schedule<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/linearschedule/index.html class=dd-item><a href=../../introduction/linearschedule/index.html>Linear Schedules<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/derivation/index.html class="dd-item active"><a href=../../introduction/derivation/index.html>Derivation<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/wavefront/index.html class=dd-item><a href=../../introduction/wavefront/index.html>Wavefronts of Computation<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/nextsteps/index.html class=dd-item><a href=../../introduction/nextsteps/index.html>Next Steps<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/introduction/prototype/index.html class=dd-item><a href=../../introduction/prototype/index.html>Prototype<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/design/index.html class="dd-item alwaysopen"><a href=../../design/index.html>Elements of Good Design<i class="fas fa-check read-icon"></i></a><ul id=subsections-d25739b54eec6c54b486058b3e2f3701><li data-nav-id=/design/currentstate/index.html class=dd-item><a href=../../design/currentstate/index.html>Algorithm Dynamics<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/design/elements/index.html class=dd-item><a href=../../design/elements/index.html>Elements of Design<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/design/dfm/index.html class=dd-item><a href=../../design/dfm/index.html>Data Flow Machine<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/design/time/index.html class=dd-item><a href=../../design/time/index.html>Time: the when<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/design/space/index.html class=dd-item><a href=../../design/space/index.html>Space: the where<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/design/nextsteps/index.html class=dd-item><a href=../../design/nextsteps/index.html>Next Steps<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/blas/index.html class="dd-item alwaysopen"><a href=../../blas/index.html>Basic Linear Algebra<i class="fas fa-check read-icon"></i></a><ul id=subsections-c04c91366fb5eba2efaaddab304f1204><li data-nav-id=/blas/level1/index.html class=dd-item><a href=../../blas/level1/index.html>BLAS Level 1<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/blas/level2/index.html class=dd-item><a href=../../blas/level2/index.html>BLAS Level 2<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/blas/level3/index.html class=dd-item><a href=../../blas/level3/index.html>BLAS Level 3<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/factorization/index.html class="dd-item alwaysopen"><a href=../../factorization/index.html>Matrix Factorization<i class="fas fa-check read-icon"></i></a><ul id=subsections-ad68331fc764b7acec0f1f237c6522bf><li data-nav-id=/factorization/factorization/index.html class=dd-item><a href=../../factorization/factorization/index.html>Matrix Factorizations<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/matrixkernels/index.html class="dd-item alwaysopen"><a href=../../matrixkernels/index.html>Matrix Kernels<i class="fas fa-check read-icon"></i></a><ul id=subsections-7c2bfc25d3abc9a62b3397c74cd33be7><li data-nav-id=/matrixkernels/matrixkernels/index.html class=dd-item><a href=../../matrixkernels/matrixkernels/index.html>Matrix Kernels<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/linearsolvers/index.html class="dd-item alwaysopen"><a href=../../linearsolvers/index.html>Linear Solvers<i class="fas fa-check read-icon"></i></a><ul id=subsections-5d80ff15816d384ff301e1b89749e656><li data-nav-id=/linearsolvers/solvers/index.html class=dd-item><a href=../../linearsolvers/solvers/index.html>Linear Solvers<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/linearsolvers/lu/index.html class=dd-item><a href=../../linearsolvers/lu/index.html>Gaussian Elimination<i class="fas fa-check read-icon"></i></a></li></ul></li></ul><div class="footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showVisitedLinks showFooter"></div><hr class="default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showVisitedLinks showFooter"><div id=prefooter class="footerLangSwitch footerVariantSwitch footerVisitedLinks showVariantSwitch showVisitedLinks"><ul><li id=select-language-container class=footerLangSwitch><div class="padding select-container"><i class="fas fa-language fa-fw"></i>
<span>&nbsp;</span><div class=select-style><label class=a11y-only for=select-language>Language</label>
<select id=select-language onchange="location=baseUri+this.value"></select></div><div class=select-clear></div></div></li><li id=select-variant-container class="footerVariantSwitch showVariantSwitch"><div class="padding select-container"><i class="fas fa-paint-brush fa-fw"></i>
<span>&nbsp;</span><div class=select-style><label class=a11y-only for=select-variant>Theme</label>
<select id=select-variant onchange=window.variants&&variants.changeVariant(this.value)><option id=blue value=blue selected>Blue</option><option id=green value=green>Green</option><option id=red value=red>Red</option><option id=relearn-light value=relearn-light>Relearn Light</option><option id=relearn-dark value=relearn-dark>Relearn Dark</option></select></div><div class=select-clear></div></div><script>window.variants&&variants.markSelectedVariant()</script></li><li class="footerVisitedLinks showVisitedLinks"><button class=padding onclick=clearHistory()><i class="fas fa-history fa-fw"></i> Clear History</button></li></ul></div><div id=footer class="footerFooter showFooter"><p>Built with <a href=https://github.com/McShelby/hugo-theme-relearn title=love><i class="fas fa-heart"></i></a> by <a href=https://gohugo.io/>Hugo</a></p></div></div></aside><script src=../../js/clipboard.min.js?1675444628 defer></script>
<script src=../../js/perfect-scrollbar.min.js?1675444628 defer></script>
<script src=../../js/featherlight.min.js?1675444628 defer></script>
<script>function useMathJax(e){if(!Object.assign)return;window.MathJax=Object.assign(window.MathJax||{},{loader:{load:["[tex]/mhchem"]},startup:{elements:[".math"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]]},options:{enableMenu:!1}},e)}useMathJax(JSON.parse("{}"))</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=../../js/jquery.svg.pan.zoom.js?1675444628 defer></script>
<script src=https://unpkg.com/mermaid/dist/mermaid.min.js defer></script>
<script>window.themeUseMermaid=JSON.parse('{ "theme": "default" }')</script><script src=https://unpkg.com/rapidoc/dist/rapidoc-min.js defer></script>
<script>window.themeUseSwagger=JSON.parse('{ "theme": "light" }')</script><script src=../../js/theme.js?1675444628 defer></script></body></html>