<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Getting Started on Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/introduction/index.html</link><description>Recent content in Getting Started on Domain Flow Architecture</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2017 06:49:44 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/introduction/index.xml" rel="self" type="application/rss+xml"/><item><title>An Example</title><link>https://stillwater-sc.github.io/domain-flow/introduction/example/index.html</link><pubDate>Wed, 15 Feb 2017 07:00:21 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/example/index.html</guid><description>Let&amp;rsquo;s look at a simple, but frequently used operator in Deep Learning inference: dense matrix multiplication. A Domain Flow program for this operator is shown below:
compute ( (i,j,k) | 1 &amp;lt;= i,j,k &amp;lt;= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } The underlying algorithm requires a domain of computation governed by a set of constraints, and a set of computational dependencies that implicitly define a partial order across all the operations in the computation.</description></item><item><title>Parallel Programming</title><link>https://stillwater-sc.github.io/domain-flow/introduction/parallel-programming/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/parallel-programming/index.html</guid><description>To appreciate the domain flow programming model and what it enables, you need to think about the physical form a &amp;lsquo;program evaluator&amp;rsquo; could take. In the days when a processor occupied the volume of a small room, any physical computational machine was limited to a single computational element. This implied that the execution of any algorithm had to be specified as a complete order in time. At each step of the execution, the computational element would read input operands, execute an instruction, and write a result.</description></item><item><title>Constraints of Spacetime</title><link>https://stillwater-sc.github.io/domain-flow/introduction/spacetime/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/spacetime/index.html</guid><description>If you visualize the &amp;lsquo;world&amp;rsquo; from the perspective of an operand flowing through a machine, you realize that a physical machine creates a specific spatial constraint for the movement of data. Processing nodes are fixed in space, and information is exchanged between nodes to accomplish some transformation. Nodes consume and generate information, and communication links move information (program and data) between nodes. This leads to a simple abstraction of a physical parallel machine consisting of a fixed graph in space that constraints computation and communication events generated by the parallel algorithm.</description></item><item><title>Computational Spacetime</title><link>https://stillwater-sc.github.io/domain-flow/introduction/computational-spacetime/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/computational-spacetime/index.html</guid><description>Data movement in a parallel machine experiences the constraints of spacetime and more. The propagation delay in a communication channel acts similarly as the constraint on the speed of light in spacetime. Let&amp;rsquo;s bring back the mental model of a parallel machine being a fixed graph in space with communication channels connecting a set of nodes. The channels of communication constrain the propagation directions for information exchange. If we make the assumption that computation and communication are separate steps and that they can be overlapped to hide the latency of the communication phase, then we can leverage the mental model of spacetime to argue about partial orders of computational events that might be able to effect each other.</description></item><item><title>Domain Flow</title><link>https://stillwater-sc.github.io/domain-flow/introduction/domain-flow/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/domain-flow/index.html</guid><description>Domain flow is an abstract parallel programming model that is invariant to technology changes.
An equation $c = a \oplus b$ is comprised of a computation phase, the $\oplus$, and a communication phase, the $=$.
Implementation technology will impact these phases differently, and we are seeking a programming model that is invariant to the difference. A thought experiment will shed light on the desired properties of such a model.
In the extreme, if the two delays are very different, then the physical execution will either be computation-bound, or communication-bound.</description></item><item><title>Free Schedule</title><link>https://stillwater-sc.github.io/domain-flow/introduction/freeschedule/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/freeschedule/index.html</guid><description>We alluded to the fact that inherently-parallel algorithms exhibit some partial order, and not a total order, because the instructions that can execute independently do not have any explicit order among each other. This extra degree of freedom is another benefit domain flow algorithms exhibit over sequential algorithms. It allows the execution engine to organize any resource contention in a more energy, space, or time efficient way, as long the machine does not violate the dependency relationships specified in the algorithm.</description></item><item><title>Linear Schedules</title><link>https://stillwater-sc.github.io/domain-flow/introduction/linearschedule/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/linearschedule/index.html</guid><description>In the previous section, we saw what the computational evolution of an unconstrained parallel algorithm looks like. However, an actual physical system would have finite resources, and certainly limited operand bandwidth.
The free schedule of a parallel algorithm tends to be unrealizable as the size of the problem grows.
Let&amp;rsquo;s go through the thought experiment what the free schedule demands from a physical system. In the free schedule animation, the propagation recurrences distributing the $A$ and $B$ matrix elements throughout the 3D lattice run &amp;lsquo;ahead&amp;rsquo; of the actual computational recurrence calculating the $C$ matrix elements.</description></item><item><title>Derivation of the matrix multiply domain flow program</title><link>https://stillwater-sc.github.io/domain-flow/introduction/derivation/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/derivation/index.html</guid><description>The concepts of partial and total orders are essential for finding optimal domain flow algorithms. Partial orders, or Poset, are the source of high-performance, low-power execution patterns.
The Linear Algebra universe is particularly rich in partial orders, something that has been exploited for centuries 1. Matrix Computations2 by Golub, and van Loan provide a comprehensive review. What follows may be a bit technical, but keep in mind the visualizations of the previous pages as you try to visualize what the math implies.</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/introduction/nextsteps/index.html</link><pubDate>Wed, 15 Feb 2017 07:44:23 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/nextsteps/index.html</guid><description>Now that we have a rudimentary understanding of parallel algorithms and their physical execution, the next step is to learn about what makes for a good/efficient parallel algorithm.</description></item><item><title>Wavefronts of Computation</title><link>https://stillwater-sc.github.io/domain-flow/introduction/wavefront/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/wavefront/index.html</guid><description>Recap of this chapter: domain flow algorithms are defined as systems of recurrence equations, which represent information propagation through a computational spacetime. This spacetime is created by the spatial organization of compute elements and their intercommunication structure. The data dependencies inherent to the recurrence equations define an implicit partial order, called the free schedule, and we can further constrain that partial order using piecewise linear schedules to minimize the need for storage elements.</description></item></channel></rss>