<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Architecture - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/index.html</link><description>A model of computation describes how an output of a mathematical function is computed given an input. These models specify how units of computation, memories, and information (data) exchanges are organized. The benefits provided by a model of computation is the measure of the computational complexity of an algorithm independent of any specific physical implementation.
There are sequential models of computation:
Finite State Machines (FSM) Pushdown automata Turing machines Decision Tree Models Random Access Machine And parallel models of computation:</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 07 Jan 2025 13:20:04 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/ch2-moc/index.xml" rel="self" type="application/rss+xml"/><item><title>Random Access Machine</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/spm/index.html</link><pubDate>Mon, 06 Jan 2025 16:43:57 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/spm/index.html</guid><description>The Random Access Machine (RAM) model of computation is a theoretical framework developed to analyze algorithms and computational efficiency. Introduced in the mid-20th century, the RAM model was devised to bridge the gap between high-level algorithmic analysis and the hardware implementation of computing systems. It simplifies the architecture of a physical computer into an idealized system that supports a sequential execution of instructions, allowing for the study of computational complexity independent of hardware idiosyncrasies.</description></item><item><title>Computer Organization</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/taxonomy/index.html</link><pubDate>Tue, 07 Jan 2025 13:20:04 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/taxonomy/index.html</guid><description>What would be the best way to build scalable, parallel execution engines?
In 1966, Michael J. Flynn, proposed a taxonomy based on two dimensions, the parallelism of data and instructions 1.
A purely sequential machine has a single instruction stream and a single data stream and the acronym SISD. A machine that applies the same instruction on multiple data elements is a SIMD machine, short for Single Instruction Multiple Data. Machines that have multiple instruction streams operating on a single data element as used in fault-tolerant and redundant system designs, and carry the designation MISD, Multiple Instruction Single Data. The Multiple Instruction Multiple Data machine, or MIMD, consists of many processing elements simultaneously operating on different data.</description></item><item><title>Data Flow Model</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfm/index.html</link><pubDate>Mon, 06 Jan 2025 16:44:37 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfm/index.html</guid><description>The Data Flow model emerged in the early 1970s through pioneering work by Jack Dennis at MIT1, paralleled by research by Robert Barton and Al Davis at the University of Utah2. This model arose as an alternative to the von Neumann architecture to create a framework for expressing parallelism. Unlike traditional von Neumann architectures, which execute instructions sequentially, the data flow model represents computation as a directed graph of data dependencies. Nodes in this graph correspond to operations, and edges represent data flowing between them. Execution is driven by the availability of data, allowing operations to proceed independently and in parallel. The data flow model was promising better parallel execution by eliminating the program counter and global updating of state that are essential in the operation of the Stored Program Machine.</description></item><item><title>Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfa/index.html</link><pubDate>Mon, 06 Jan 2025 16:46:11 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfa/index.html</guid><description>With the advent of Very Large Scale Integration (VLSI), it became apparent that the control mechanism of the Stored Program Machine (SPM) to manage resource contention was not well-suited to the characteristics of VLSI 1. VLSI offers large amount of hardware at very low cost, but interconnections between the logic devices are as expensive as the logic devices themselves for all three metrics: area, propagation delays, and energy. Effective use of VLSI technology is only achieved when the computational resource organization is constructed with local interconnections.</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/nextsteps/index.html</link><pubDate>Mon, 06 Jan 2025 16:49:53 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/nextsteps/index.html</guid><description>We have quickly introduced different computer hardware organizations to deliver resource contention management required for the execution of algorithms that exhibit problem sizes larger than the available hardware resources. Our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.</description></item></channel></rss>