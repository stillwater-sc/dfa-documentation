<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Resource Contention Management - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/index.html</link><description>A model of computation is a model which describes how an output of a mathematical function is computed given an input. These models describe how units of computation, memories, and information (data) exchanges are organized. The benefits provided by a model of computation is the measure of the computational complexity of an algorithm independent of any specific physical implementation.
There are sequential models of computation:
Finite State Machines (FSM) Pushdown automata Turing machines Decision Tree Models Random Access Machine And parallel models of computation:</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 06 Jan 2025 16:49:53 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/ch2-moc/index.xml" rel="self" type="application/rss+xml"/><item><title>Stored Program Machine</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/spm/index.html</link><pubDate>Mon, 06 Jan 2025 16:43:57 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/spm/index.html</guid><description>After loading a program into the main memory of the Stored Program Machine, the Operating System writes the address of the entry point of the program into the Instruction Pointer (IP) register of the processor. After that initialization, a Stored Program Machine uses the following resource contention management mechanism to unabiguously execute a program:
fetch an instruction from the address pointed to by the IP register, and update IP to point to the next instruction decode instruction dispatch to appropriate execution units 4a. if execute unit is the load/store unit then request data from a memory location or provide data to store at memory location 4b. if execute unit is branch unit then load IP with new address 4c. else execute address/branch/arithmetic/logic/function operation store result of execute unit in register file This cycle is repeated till a halt instruction is executed, or an interrupt is issued.</description></item><item><title>Data Flow Machine</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfm/index.html</link><pubDate>Sat, 06 Jan 2024 16:44:37 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfm/index.html</guid><description>In the late 60’s and 70’s when computer scientists were exploring parallel computation by building the first parallel machines and developing the parallel algorithm complexity theory, folk realized that this over-constrained specification was a real problem for concurrency. One proposal to rectify this was a natively parallel execution model called the Data Flow Machine (DFM). A Data Flow Machine uses a different resource contention management mechanism:
write an operand into an appropriate operand slot in an instruction token stored in a Content Addressable Memory (CAM) by an instruction tag check if all operands are present to start the execution cycle of the instruction if an instruction is ready then extract it from the CAM and inject it into a fabric of computational elements deliver the instruction to an available execution unit execute the instruction, and finally write the result back into an operand slot in target instruction token stored in the CAM The strength of the resource contention management of the Data Flow Machine is that the machine can execute along the free schedule, that is, the inherent parallelism of the algorithm. Any physical implementation, however, is constrained by the energy-efficiency of the CAM and the network that connects the CAM to the fabric of computational elements. As concurrency demands grow the efficiency of both the CAM and the fabric decreases making large data flow machines unattractive. However, small data flow machines don’t have this problem and are able to deliver energy-efficient, low-latency resource management. Today, all high-performance microprocessors have a data flow machine at their core.</description></item><item><title>Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfa/index.html</link><pubDate>Sat, 06 Jan 2024 16:46:11 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/dfa/index.html</guid><description>Domain Flow Architecture (DFA) machines are the class of machines that execute using the domain flow execution model. The fundamental problem limiting the energy efficiency of the data flow machine is the size of the CAM and fabric. As they are managed as two separate clusters of resources, they grow together. The domain flow execution model recognizes that for an important class of algorithms, we can distribute the CAM across the computational elements in the fabric, and we can scale the machine without negatively impacting the cycle time of execution.</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/ch2-moc/nextsteps/index.html</link><pubDate>Mon, 06 Jan 2025 16:49:53 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch2-moc/nextsteps/index.html</guid><description>We have quickly introduced computer hardware organization to deliver resource contention management. Our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.</description></item></channel></rss>