<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Basic Linear Algebra - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch4/index.html</link><description>Basic Linear Algebra Subroutines are an historically significant set of functions that encapsulate the basic building blocks of a large collection of linear algebra algorithms and implementation.
The BLAS library has proven to be a very productive mechanism to create and disseminate highly optimized numerical libraries to a plethora of computer architectures and machines. Writing high-performance linear algebra algorithms turns out to be a tenacious problem, but since linear algebra operations are essential
components in computational methods, the investment can pay high dividends.</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2017 07:55:23 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/ch4/index.xml" rel="self" type="application/rss+xml"/><item><title>BLAS Level 1</title><link>https://stillwater-sc.github.io/domain-flow/ch4/level1/index.html</link><pubDate>Wed, 15 Feb 2017 07:55:08 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch4/level1/index.html</guid><description>BLAS Level 1 are $\mathcal{O}(N)$ class operators. This makes these operators operand access limited and thus require careful distribution in a parallel environment.
There are four basic vector operations, and a fifth convenience operators. Let $ \alpha \in \Bbb{R}, x \in \Bbb{R^n}, y \in \Bbb{R^n}, z \in \Bbb{R^n}$$ then:
vector scale: scalar-vector multiplication: $z = \alpha x \implies (z_i = \alpha x_i)$ vector element addition: $z = x + y \implies (z_i = x_i + y_i)$ vector element multiply: $z = x * y \implies (z_i = x_i * y_i)$ vector dot product: $c = x^Ty \implies ( c = \sum_{i = 1}^n x_i y_i ) $, aka inner-product saxpy, or scalar alpha x plus y, $z = \alpha x + y \implies z_i = \alpha x_i + y_i $ The fifth operator, while technically redundant, makes the expressions of linear algebra algorithms more concise.</description></item><item><title>BLAS Level 2</title><link>https://stillwater-sc.github.io/domain-flow/ch4/level2/index.html</link><pubDate>Wed, 15 Feb 2017 07:55:18 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch4/level2/index.html</guid><description>BLAS Level 2 are $\mathcal{O}(N^2)$ class operators, still operand access limited as we need to fetch multiple operands per operation without any reuse. The core operator is the matrix-vector multiplication in all its different forms specialized for matrix shape — triangular, banded, symmetric —, and matrix type — integer, real, complex, conjugate, or Hermitian —.
Let $A \in \Bbb{R^{mxn}}$, the matrix-vector product is defined as: $$z = Ax, \space where \space x \in \Bbb{R^n}$$</description></item><item><title>BLAS Level 3</title><link>https://stillwater-sc.github.io/domain-flow/ch4/level3/index.html</link><pubDate>Wed, 15 Feb 2017 07:55:23 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch4/level3/index.html</guid><description>BLAS Level 3 are $\mathcal{O}(N^3)$ operators, and finally compute bound creating many opportunities to optimize operand reuse.
In addition to matrix-matrix multiply there are the Rank-k update operators, which are outer-products and matrix additions.
Here is a Hermitian Rank-k update:
$$ C = \alpha A A^T + \beta C, \space where \space C \space is \space Hermitian. $$ A Hermitian matrix is defined as a matrix that is equal to its Hermitian conjugate. In other words, the matrix $C$ is Hermitian if and only if $C = C^H$. Obviously a Hermitian matrix must be square. Hermitian matrices can be understood as the complex extension of real symmetric matrices.</description></item></channel></rss>