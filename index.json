[{"content":"A memory access in a physical machine can be very complex: for example, think about what happens when a program accesses an operand located at an address that is not in physical memory. This is referred to as a page miss. The performance difference between an access from the local L1 cache versus a page miss can be 8 orders of magnitude. An L1 cache hit is serviced in the order of 500pico-seconds, whereas a page miss can be as slow as 15 milli-seconds if it has to come from rotational storage.\nFor web applications, the elements of good design mandate that the algorithm can service all requests from memory. For high-performance computing, the mandate is even strong. In addition of requiring that all requests are serviced by memory, it is also paramount that the algorithm keeps all the processors busy by correct load balancing and overlapping operand accesses with computation. This is particularly important when operands need to traverse the weakest link in a parallel computation: the network.\nThus good algorithms, sequential or parallel, take into account where, and how, operands need to be accessed. The trick in sequential algorithm is to be aware of the memory hierarchy. But the trick for parallel algorithms is to be aware where in space the operands are, and to avoid having to access remote operands.\nVariability is another attribute that parallel algorithms are sensitive to. As thousands of processors are working together, when the initial division of labor has generated a workload for each processor that is balanced, any variability, which can be caused by poor operand access locality, networking contention, or clock frequency modulation due to power constraints, causes the collective to have to wait for the slowest process. As the number of processors grows, so does the variability, and with it the utilization drops.\n","description":"","tags":null,"title":"Algorithm Dynamics","uri":"/design/currentstate/index.html"},{"content":"Domain Flow Algorithms Domain Flow algorithms are parallel algorithms that incorporate the constraints of space and time. By honoring the delay that is inherent to exchanging information between two spatially separate computation or storage sites, domain flow algorithms can improve performance and energy efficiency compared to sequential programming models that depend on (globally addressable) random access memory.\nHigh-performance, low-latency, energy-efficient computation is particularly important for the emerging application class of autonomous intelligent systems.\n","description":"","tags":null,"title":"Getting Started","uri":"/introduction/index.html"},{"content":"We can summarize the attributes of good parallel algorithm design as\nlow operation count, where operation count is defined as the sum of operators and operand accesses minimal operand movement minimal resource contention Item #1 is well-known by theoretical computer scientists.\nItem #2 is well-known among high-performance algorithm designers.\nItem #3 is well-known among hardware designers and computer engineers.\nWhen designing domain flow algorithms, we are looking for an energy efficient embedding of a computational graph in space, and it is thus logical that we need to combine all three attributes of minimizing operator count, operand movement, and resource contention. The complexity of #3 is what makes hardware design so much more complex and slow as validation of the actual execution is taking a disproportional amount of time. But the complexity of operator contention can be mitigated by clever resource contention management.\nThe Stored Program Machine (SPM) is an example of a specific resource contention management mechanism designed specifically for one central processing unit shared by the complete computational graph. The SPM protocol to avoid contention on that one processing resource is described as follows:\nread an instruction from memory, decode the instruction, fetch the input operands from memory, execute the instruction using the input operands write back the result to memory But the Stored Program Machine isn’t particularly energy efficient, and its resource contention management is over-constrained as it forces a total order on the computation graph. This tasks of creating the total order falls on the algorithm designer.\n","description":"","tags":null,"title":"Elements of Design","uri":"/design/elements/index.html"},{"content":"An Example Let’s look at a simple, but frequently used operator in Deep Learning inference: dense matrix multiplication. A Domain Flow program for this operator is shown below:\ncompute ( (i,j,k) | 1 \u003c= i,j,k \u003c= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } The underlying algorithm requires a domain of computation governed by a set of constraints, and a set of computational dependencies that implicitly define a partial order across all the operations in the computation. The partial order is readily visible in the need to have computed the result for $c[i,j,k-1]$ before the computation of $c[i,j,k]$ can commence. In contrast, the $a$ and $b$ recurrences are independent of each other.\nFrom a design perspective, an explicit dependency enables us to ‘order’ the nodes in a computational graph. This can be done in time, as is customary in sequential programming: the sequence of instructions is a constraint to order the operations in time and enable an unambiguious semantic interpretation of the value of a variable even though that variable may be reused. Parallel algorithms offer more degrees of freedom to order the computational events. In addition to sequential order, we can also disambiguate variables in space. For high-performant parallel computation, we are looking for partial orders, or posets, where independent computational events are spatially separated in space, and where dependent events are spatially ‘close’.\nIf we look back again at the domain flow code of matrix multiply, we observe that all results are assigned to a unique variable. This is called Single Assignment Form (SAF), and this yields a computational graph that makes all computational dependencies explicit.\nThe second observation is that the computational events are made unique with a variable name and an index tag, represented by $[i,j,k]]$. The constraint set: $compute ( (i,j,k) | 1 \u003c= i,j,k \u003c= N )$, carves out a subset in the lattice $N^3$, and the body defines the computational events at each of the lattice points $[i,j,k]$ contained in the subset.\nThirdly, dependencies between computational events are specified by an index expression. The statement $a: a[i,j-1,k]$ is a shorthand for $a: a[i,j,k] =\u003e a[i,j,k] = a[i,j-1,k]$, defining a dependency to the value at $[i,j-1,k]$ for each lattice point where the variable $a$ is defined.\nA thorough understanding of the partial and total orders inherent in the parallel computation is essential for finding optimal domain flow algorithms.\nHigh-performance, low-power execution patterns frequently involve a partial order that enables timely reuse of computational results, or creates flexibility to organize just-in-time arrival of input operands to avoid memory elements.\nIn the next segment, let’s explore these execution patterns.\n","description":"","tags":["domain-flow","algorithm","matrix-multiply"],"title":"An Example","uri":"/introduction/example/index.html"},{"content":"In the late 60’s and 70’s when computer scientists were exploring parallel computation by building the first parallel machines and developing the parallel algorithm complexity theory, folks realized that this over-constrained specification was a real problem. One proposal to rectify this sequential straight jacket was a natively parallel execution model called the Data Flow Machine (DFM). A Data Flow Machine uses a very different resource contention management mechanism:\nwrite an operand to an instruction slot of an instruction token stored in a Content Addressable Memory (CAM) check if all operands are present to start the execution cycle if an instruction is ready then extract it from the CAM and inject it into a fabric of computational elements deliver the instruction to an execute unit execute the instruction, and write the result back into an instruction slot in the CAM The strength of the resource management of the Data Flow Machine is that the machine can execute along the free schedule, or inherent parallelism, of the algorithm. Any physical implementation however, is constrained by the energy efficiency of the CAM and the network that connects the CAM to the fabric of computational elements. As concurrency demands grow the efficiency of both the CAM and the fabric decreases making large data flow machines unattractive. However, small data flow machines don’t have this problem and are thus very attractive to deliver energy-efficient, low-latency resource management. All high-performance microprocessors today have a data flow machine at its core.\nThe Domain Flow Architecture are the class of machines that execute using the domain flow execution model. The fundamental problem limiting the energy efficiency of the data flow machine is the size of the CAM and fabric. As they are managed as two separate clusters of resources, they grow together. The domain flow execution model recognizes that for an important class of algorithms, we can distribute the CAM across the computational elements in the fabric, and we can scale the machine without negatively impacting the cycle time of execution.\nThe Domain Flow execution model solves the complexity of minimizing operator contention. As long as the domain flow algorithm does not map computational events on the same location in space, the physical relationships between the computational events are kept invariant in the physical execution realm. This translates into the requirement for good domain flow algorithms to design parallel algorithms that exhibit partial orders that are regular and are separated in space. That is a mouthful, but we can make this more tangible when we discuss in more detail the temporal behavior of a domain flow program in the next section about time.\n","description":"","tags":null,"title":"Data Flow Machine","uri":"/design/dfm/index.html"},{"content":"Lorem Ipsum\n","description":"","tags":null,"title":"Time: the when","uri":"/design/time/index.html"},{"content":"Parallel Programming To appreciate the domain flow programming model and what it enables, you need to think about the physical form a ‘program evaluator’ could take. In the days when a processor occupied the volume of a small room, any physical computational machine was limited to a single computational element. This implied that the execution of any algorithm had to be specified as a complete order in time. At each step of the execution, the computational element would read input operands, execute an instruction, and write a result. The reading and writing of operands was from and to some storage mechanism.\nThis sequential approach has been very successful, as it is a general purpose mechanism to execute any algorithm. But it is not the most efficient approach to execute all algorithms. This is the niche that domain flow fills: algorithms that exhibit complex, inherently parallel, but geometrically constrained concurrency patterns. These algorithms offer the opportunity to be evaluated more efficiently by taking advantage of the regularity of movement of collections of data elements, dubbed domains. The venerable matrix multiply (matmul) is a good introduction to this class of algorithms, more formally defined by the term systems of affine recurrence equations. Matmul is so regular that the affine index transformation is the identity matrix: matmul can be expressed as a system of regular recurrence equations.\nThe Domain Flow programming model was invented in the late 80’s to solve the problem of parallel algorithm dependence on the structure of the underlying hardware. This period generated many new and wonderful parallel machines:\nTransputer Hypercubes from nCUBE and Intel the first SMPs of Sequent and NUMA innovations, the first massively parallel machines, CM-1 and CM-2 from Thinking Machines The software engineers tasked to write high-performance libraries for these machines discovered the inconvenient truth about programming for high-performance: the characteristics of the hardware drive the structure of the optimal algorithm. The best algorithm for our matrix multiply example has four completely different incarnations for the machines mentioned above. Furthermore, the optimal algorithm even changes when the same machine architecture introduces a new, typically faster, implementation. And we are not just talking about simple algorithmic changes, such as loop order or blocking, sometimes even the underlying mathematics needs to change.\nGiven the complexity of writing parallel algorithms, this one-off nature of parallel algorithm design begged the question: is there a parallel programming model that is invariant to the implementation technology of the machine?\n","description":"","tags":["domain-flow","matrix-multiply","index-space","lattice"],"title":"Parallel Programming","uri":"/introduction/parallel-programming/index.html"},{"content":"BLAS Level 1 are $\\mathcal{O}(N)$ class operators. This makes these operators operand access limited and thus require careful distribution in a parallel environment.\nThere are four basic vector operations, and a fifth convenience operators. Let $ \\alpha \\in \\Bbb{R}, x \\in \\Bbb{R^n}, y \\in \\Bbb{R^n}$ then:\nscalar-vector multiplication: $z = \\alpha x \\space (z_i = \\alpha x_i)$ vector addition: $z = x + y \\space (z_i = x_i + y_i)$ dot product: $c = x^Ty \\space ( c = \\sum_{i = 1}^n x_i y_i ) $, aka inner-product vector multiply: $z = x .* y \\space (z_i = x_i * y_i)$ $saxpy$, or scalar alpha x plus y, $z = \\alpha x + y \\implies z_i = \\alpha x_i + y_i $ The fifth operator, while technically redundant, makes the expressions of linear algebra algorithms more productive.\nOne class of domain flow programs for these operators assumes a linear distribution of the vectors, and propagation recurrences for scalar entities. For the dot product, we also use a propagation recurrence to produce the scalar result.\nscalar-vector multiplication compute ( (i,j,k) | 1 \u003c= i \u003c= N, j = 1, k = 1 ) { alpha: alpha[i-1,j,k) z: alpha[i-1,j,k] + z[i,j-1,k) } vector addition compute ( (i,j,k) | 1 \u003c= i \u003c= N, j = 1, k = 1 ) { z: x[i,j-1,k] + y[i,j,k-1) } dot product compute ( (i,j,k) | 1 \u003c= i \u003c= N, j = 1, k = 1 ) { x: x[i,j-1,k] y: y[i,j,k-1] z: x[i,j,k] + y[i,j,k) } vector multiplication compute ( (i,j,k) | 1 \u003c= i \u003c= N, j = 1, k = 1 ) { alpha: alpha[i-1,j,k) z: alpha[i-1,j,k] * x[i,j-1,k) } SAXPY compute ( (i,j,k) | 1 \u003c= i \u003c= N, j = 1, k = 1 ) { alpha: alpha[i-1,j,k) z: alpha[i-1,j,k] * x[i,j-1,k) + y[i,j,k-1) } ","description":"","tags":null,"title":"BLAS Level 1","uri":"/blas/level1/index.html"},{"content":"Lorem Ipsum\n","description":"","tags":null,"title":"Space: the where","uri":"/design/space/index.html"},{"content":"Constraints of Spacetime If you visualize the ‘world’ from the perspective of an operand flowing through a machine, you realize that a physical machine creates a specific spatial constraint for the movement of data. Processing nodes are fixed in space, and information is exchanged between nodes to accomplish some transformation. Nodes consume and generate information, and communication links move information (program and data) between nodes. This leads to a simple abstraction of a physical parallel machine consisting of a fixed graph in space that constraints computation and communication events generated by the parallel algorithm.\nJust like it takes time to compute, it takes time to communicate. In an expression, $c = a + b$, we implicitly know that the add operation takes some time to complete. However, the assignment operation, which represents the exchange of information and is equivalent to the communication phase, takes time just the same. The time relationship between computation and communication delay impacts the structure of an optimal parallel algorithm.\nTechnology impacts the performance of both computation and communication, but in very different ways. Typically, computational performance, being governed by miniaturization of VLSI manufacturing process geometries, improves more rapidly than communication performance. For large scale computation communication performance tends to be governed by system integration constraints that are external to the chip, but with submicron manufacturing process nodes, even on-chip communication delays are impacting computational performance.\nIf we revisit the spatial graph of a parallel machine in our thought experiment, an improvement in the performance of the computation relative to the communication of information changes the trade-off between computation and communication in the parallel algorithm. In the vocabulary of spacetime, remote nodes move farther away.\nSpacetime has a geometric interpretation of the relationship between space and time, called the spacetime light cone, as shown here:\nSpacetime Light Cone The spacetime light cone is a visual mnemonic device to think about which events in spacetime can affect each other. Parallel computation requires the distribution of information and is thus constrained by the propagation of information. A computational event has to be able to ‘see’ its operands before it can commence. Otherwise stated, its operands need to lie in the future light cone.\nThese temporal constraints are further complicated by the fact that man-made structures today do not communicate through free space yet, and the physical communication structure adds additional constraints on the shape and extend of the future cone.\nThese man-made computational structures are dubbed computational spacetimes.\n","description":"","tags":["computational-spacetime"],"title":"Spacetime","uri":"/introduction/spacetime/index.html"},{"content":"In this short introduction to parallel algorithms in general and domain flow in particular, our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.\nOnce we get a good collection of fast, and energy efficient algorithms together, we can start to explore how best to engineer combinations of these operators. We will discover that sometimes, the cost of an information exchange makes a whole class of algorithms unattractive for parallel executions. With that comes the need to create new algorithms and sometimes completely new mathematical approaches to properly leverage the available resources.\n","description":"","tags":null,"title":"Next Steps","uri":"/design/nextsteps/index.html"},{"content":"Computational Spacetime Data movement in a parallel machine experiences the constraints of spacetime and more. The propagation delay in a communication channel acts similarly as the constraint on the speed of light in spacetime. Let’s bring back the mental model of a parallel machine being a fixed graph in space with communication channels connecting a set of nodes. The channels of communication constrain the propagation directions for information exchange. If we make the assumption that computation and communication are separate steps and that they can be overlapped to hide the latency of the communication phase, then we can leverage the mental model of spacetime to argue about partial orders of computational events that might be able to effect each other. We call this the computational spacetime of the machine. An operand cannot be delivered to remote destination unless it falls inside the future cone of the computational spacetime outlined by the distance an operand can travel along the communication channels in some unit of time.\nThis dynamic is shown in the following animation: the expanding time horizon of a computational event will be able to trigger dependent computations at a distance.\nbrowser doesn't support canvas tags As designers, we have the option to try to compute more, and thus take more time, to broaden the computational spacetime cone and be able to exchange information to nodes that are farther away. This is the act of blocking algorithms if you are an algorithm designer, and it is the act of building big processors with lots of storage if you are a computer designer. But we also have the option to compute less, and organize the operands in space so that they fall in the future cone of the computational event. That is the foundation of designing domain flow algorithms, and building fine-grain computational fabrics.\nNeither of these approaches is a panacea for parallel computation for the simple reason that different algorithms have different bottlenecks. Some are compute-bound, in which case the efficiency of computation will govern the optimal algorithm. And others are communication-bound, in which case the efficiency of information exchange is the deciding factor. Examples of communication-bound algorithms are sorting, and similar complete information exchanges such as min/max and normalize. Sometimes, a few big nodes with big communication pipes is the best organization, and sometimes, a network of tightly-coupled multiply-add functional units is better. However, if you are looking for energy efficiency, smaller tends to be better.\n","description":"","tags":["domain-flow","matrix-multiply","index-space","lattice","computational-spacetime"],"title":"Computational Spacetime","uri":"/introduction/computational-spacetime/index.html"},{"content":"Domain Flow Domain flow is an abstract parallel programming model that is invariant to technology changes.\nAn equation $c = a \\oplus b$ is comprised of a computation phase, the $\\oplus$, and a communication phase, the $=$.\nImplementation technology will impact these phases differently, and we are seeking a programming model that is invariant to the difference. A thought experiment will shed light on the desired properties of such a model.\nIn the extreme, if the two delays are very different, then the physical execution will either be computation-bound, or communication-bound. In either case, there is limited concurrency. If our technology delivers a ten-fold improvement in computation time, any dependent computational event needs to come ten times closer to maintain the same balance. If the communication phase does not improve, the computation resource will now be 90% idle as it is waiting for operands to be delivered. Our 10x technology improvement in computational performance would deliver only a 2x system-level improvement.\nMaximum concurrency emerges when the two delays are similar and can overlap each other. When the computation and communication phases are balanced we can deliver 100% resource utilization and maximum concurrency: all the electrons in the system are wiggling to affect some result.\nNow, imagine that we have as many computational resources as there are nodes in the computational graph. In that case, we could simply embed the computational graph in free space. However, a physical machine will have some physical extent, and a collection of manufactured computational nodes will fill free space in some ‘regular’, crystalline pattern. These crystalline patterns are typically referred to as a lattice. When engineering these structures, we can balance the computation and communication delays through simple area-time trade-offs. This presents the invariance we are seeking. Independent of technology, we can always engineer a machine that offers balance between computation and communication delays, and that offers connectivity among nodes that fall within the future cone of the computational spacetime.\nOne such computational spacetime that is uniform in all directions is the Cartesian lattice, $\\mathbb{N}^3$. And the design of a domain flow algorithm is the act of finding an efficient embedding of the computational graph of the single assignment form in the Cartesian lattice, $\\mathbb{N}^c$.\nBack to our matrix multiply; we can now reinterpret the domain flow algorithm as an embedding. Each index range, that is, the $i, j, k$ in the constraint set, can be seen as a dimension in the Cartesian lattice. The index tag, such as $[i,j,k]$, is a location in the lattice.\nThis is what the lattice for matmul looks like for a given N:\n","description":"","tags":["domain-flow","matrix-multiply","index-space","lattice"],"title":"Domain Flow","uri":"/introduction/domain-flow/index.html"},{"content":"Free Schedule We alluded to the fact that inherently-parallel algorithms exhibit some partial order, and not a total order, because the instructions that can execute independently do not have any explicit order among each other. This extra degree of freedom is another benefit domain flow algorithms exhibit over sequential algorithms. It allows the execution engine to organize any resource contention in a more energy, space, or time efficient way, as long the machine does not violate the dependency\trelationships specified in the algorithm.\nTypically, the complete order defined by sequential algorithms over-constrains the execution order, and parallelizing compilers can’t recover the inherent dependency structure of the mathematics behind the algorithm, leading to disappointing speed-ups. This is a fundamental limitation to trying to generate parallel execution models from sequential specifications. Secondly, as we’ll see shortly, the best parallel algorithms may organize their computation differently as compared to a sequential algorithm. There are even cases where a parallel algorithm is better off using a different mathematical basis for its solution to reduce operand movement communication-avoiding linear algebra, or higher-order methods in finite element and finite volume methods to increase the computation to operand bandwidth ratio of the kernels.\nInstead, the domain flow specification only specifies the data dependencies inherent to the algorithm: the partial order will simply evolve from these constraints. If we present a domain flow algorithm with infinite resources and instantaneous access to its inputs, then the computational activity of the specification would evolve in what is called the free schedule.\nThe free schedule for our matrix multiply is visualized in the following, interactive, simulation:\n_ We see the activity wavefront of the $a$ recurrence (blue), the $b$ recurrence (purple), and the $c$ recurrence (red) evolve through space and time.\nThe $a$ recurrence is defined by the recurrence equation: $a: a[i,j-1,k]$ is independent of both $b$ and $c$. The computational wavefront represents the computational event set: $a[i,j,k] = a[i,j-1,k]$ and will evolve along the $[0,1,0]$ direction.\nSimilarly, the $b$ recurrence, defined by the equation: $b: b[i-1,j,k]$ is independent of $a$ and $c$ and the computational wavefront evolves along the $[1,0,0]$ direction.\nThe $c$ recurrence, however, does depend on both $a$ and $b$, as well as on its own previous values. The free schedule that the $c$ recurrence evolves into is a wavefront that moves along the $[1,1,1]$ direction. The $a$ and $b$ values will arrive ’early’ at a $[i,j,k]$ lattice location, and as the $c$ values arrive, the recurrence equation for $c$, shown below, will trigger:\n$c: c[i,j,k-1] + a[i,j-1,k] * b[i-1, j, k]$ ","description":"","tags":["domain-flow","matrix-multiply","free-schedule"],"title":"Free Schedule","uri":"/introduction/freeschedule/index.html"},{"content":"Linear Schedules In the previous section, we saw what the computational evolution of an unconstrained parallel algorithm looks like. However, an actual physical system would have finite resources, and certainly limited operand bandwidth.\nThe free schedule of a parallel algorithm tends to be unrealizable as the size of the problem grows.\nLet’s go through the thought experiment what the free schedule demands from a physical system. In the free schedule animation, the propagation recurrences distributing the $A$ and $B$ matrix elements throughout the 3D lattice run ‘ahead’ of the actual computational recurrence calculating the $C$ matrix elements.\nThe A and B matrix elements arrive at their destination earlier than when they are consumed. A physical system would need to have memory to hold these operands until all of the operands are present and the computation can commence. The extra memory required to hold these operands is consuming space and energy; attributes an algorithm designer would want to optimize.\nLooking more closely at the wavefront that expresses the evolution of the C matrix elements, we can observe that the wavefront evolves as a two-dimensional plane with normal $[1 1 1]^T$. This implies that if we constrain the A and B propagation to evolve along this same wavefront then all memory requirements would disappear as we deliver the matrix elements just in time to participate in the computational event:\n$$c: c[i,j,k-1] + a[i,j-1,k] * b[i-1, j,k]$$ This constrained, linear schedule is shown in the next animation.\n_ This particular schedule is called memoryless, that is, no memory is required to execute along this evolution. Another way to look at this is that the memory function is provided by the network and the act of communicating operands between locations in the lattice. From an energy perspective, this is attractive as no additional energy is required to read or write from scratch memories that are needed just to align operand timing. As mentioned before, the operands are delivered to the computation when they can be consumed.\nAnother observation we can make is that this memoryless, linear schedule exhibits $O(N^2)$ concurrency. The index space is $O(N^3)$, but the concurrency of the algorithm is just $O(N^2)$. This implies that we can create a custom execution engine for this execution pattern that only uses $O(N^2)$ resources and would still be unconstrained.\nIf we compare the concurrency requirements of the free schedule with our memoryless, linear schedule we see that the free schedule exhibits resource requirements of the order of $O(N^3)$: the $a$ and $b$ recurrences race ahead of the computation and occupy resources as they are waiting for the computation to consume them. The free schedule is interesting from a theoretical perspective as it shows us the unconstrained evolution, or inherent concurrency, of the algorithm. But for building an actual, efficient computational engine, the free schedule tends to be too expensive. The exception, of course, is when the size of the problem matches the number of hardware resources available. In these cases, we can instantiate the complete computational graph in hardware. This is not uncommon for signal processing applications, and clever pipelining of multiple problems on the same hardware can improve utilization.\n","description":"","tags":["domain-flow","matrix-multiply","linear-schedule"],"title":"Linear Schedules","uri":"/introduction/linearschedule/index.html"},{"content":"Lorem Ipsum\n","description":"","tags":null,"title":"Linear Solvers","uri":"/linearsolvers/solvers/index.html"},{"content":"BLAS Level 2 are $\\mathcal{O}(N^2)$ class operators, typically still very much operand access limited as we need to fetch multiple operands per operation without any reuse. The core operator is the matrix-vector multiplication in all its different forms specialized for matrix shape, such as triangular, banded, symmetric, and matrix type integer, real, complex, conjugate, or Hermitian.\nLet $A \\in \\Bbb{R^{mxn}}$, the matrix-vector product is defined as: $$z = Ax, \\space where \\space x \\in \\Bbb{R^n}$$\nThe typical approach is to evaluate the dot products: $$z_i = \\sum_{j=1}^n a_{ij}x_i$$ And in a parallel environment, all these dot products are independent and conceivably could be evaluated at the same time. However, the distribution of the vector $x$ is not instantaneous, and in a balanced computation/communication architecture, we can use a propagation recurrence to distribute the vector across the matrix $A$.\nMatrix-vector multiplication compute ( (i,j,k) | 1 \u003c= i,j \u003c= N, k = 1 ) { x: x[i,j-1,k] z: a[i,j,k-1] * x[i,j-1,k] } Banded, symmetric, and triangular versions simply alter the constraint set of the domains of computation: the fundamental dependencies do not change.\n","description":"","tags":null,"title":"BLAS Level 2","uri":"/blas/level2/index.html"},{"content":"Derivation of the matrix multiply domain flow program The concepts of partial and total orders are essential for finding optimal domain flow algorithms. Partial orders, or Poset, are the source of high-performance, low-power execution patterns.\nThe Linear Algebra universe is particularly rich in partial orders, something that has been exploited for centuries. 1 Golub, and van Loan [2](#matrix computations) provide a comprehensive review. What follows may be a bit technical to communicate in mathematical terms what is going on, but keep in mind the visualizations of the previous pages as you try to visualize what the math implies.\nWe want to evaluate the matrix-matrix multiplication: $ C = AB $, where $A$, $B$, and $C$ are matrices of size $N \\times N$. We picked the square matrix version because it is simpler, but all that will follow will work just as well when the matrices are rectangular.\nMatrix operations exhibits many independent operations. For example, there are four basic vector operations:\nscale add multiply, and dot product The operator $z = alpha * x + y$ is frequently used, and although redundant, tends to be added as the fifth operator, and referred to as the saxpy operator for “Scalar Alpha X Plus Y”. The dot product is also referred to as the inner product. The inner product is an operator that collapses two vectors into a scalar. The outer product is an operator that expands two vectors into a matrix: for vector $x$ and $y$, the outer product $\\times$ is defined as: $xy^T$.\nMatrix operations can be expressed in terms of these vector operators with many degrees of freedom. For example, ‘double loop’ matrix-vector multiplication can be arranged in $2! = 2$ different ways; we can evaluate the inner products, or we can evaluate the outer products.\n‘Triple loop’ matrix-matrix multiplication can be arranged in $3! = 6$ ways. These arrangements have their own operators and their own modes of access, and thus the interplay with a spatial distribution of the rows and columns of the matrices is key to evaluate the efficiency of the computation. The orderings and properties of these different arrangements is shown in Table 1:\nLoop order Inner Loop Middle Loop Inner Loop Data Access ijk dot vector $\\times$ matrix A by row, B by column jik dot matrix $\\times$ vector A by row, B by column ikj saxpy row gaxpy B by row jki saxpy column gaxpy A by column kij saxpy row outer product B by row kji saxpy column outer product A by column Table 1: Matrix Multiplication: Orderings and Properties (see [2](#matrix computations))\nThe scale, add, and multiply operators are highly parallel in that each individual vector element operation is independent of each other. The dot product adds a consolidation, or contraction phase to yield a single, scalar valued result.\nIn a parallel context, all these vector operators have an information distribution phase that is non-trivial. First, vectors must be embedded in space, and secondly, the vectors need to be aligned so that these element operations can commence. Progressing to matrix operators, we have vectors of vectors that need to be aligned. For the domain flow algorithm we demonstrated, we started from the matrix-multiply expression as $N^2$ independent dot products. In mathematical terms: if we partition the $A$ matrix in rows, and the $B$ matrix in columns:\n$$A = \\begin{bmatrix} a_1^T \\\\\\\\ a_2^T \\\\\\\\ \\vdots \\\\\\\\ a_n^T \\end{bmatrix}$$ and\n$$B = \\begin{bmatrix} b_1 \u0026 b_2 \u0026 \\cdots \u0026 b_n \\end{bmatrix}$$ then matrix multiplication can be expressed as the collection of dot products:\n$$\\text{for i = 1:N} \\\\\\\\ \\qquad \\text{for j = 1:N} \\\\\\\\ \\qquad\\qquad c_{ij} = a_i^T b_j $$ Looking just at the individual dot product, a theoretical computer scientist would say: the fastest way to evaluate a dot product is through a binary tree of depth $log(N)$ yielding the result in $log(N)$ steps. A spec is written and handed off to a hardware engineer. When the hardware engineer looks at this problem, a very different view emerges. In the hardware world, an algebraic operator such as multiply or add evaluates, depending on the number system, in the order of 1 nsec. But sending the result across even a modestly sized chip, say 10x10 mm, can take 10 times as long. If the result needs to be communicated across the network, it can take a 1,000,000 times longer. With modern chip technology, it takes about the same time to compute a multiply or add as it does to communicate the result to a local neighborhood. From the perspective of electrons participating in the evaluation of an algebraic operator, computation and communication are roughly equal in terms of time and thus distance these electrons can ‘drift’.\nWhat this means for evaluating the dot product is that the evaluation of $$c_{ij} = \\sum\\limits_{k=1}^N a_{ik} * b_{kj}$$ can be executed efficiently as a propagation through local neighborhoods of communicating functional units. In mathematical terms we can write this as a recurrence: $$c_{ijk} = c_{ijk-1} + a_{ik} * b_{kj}$$ You can start to recognize the domain flow algorithm as presented in our example. However, if we distribute the $c_{ij}$ propagation in that $k$-dimension, then accesses to $a_{ik}$ and $b_{kj}$ are not local at all. Is there a mechanism to get the correct $a$ and $b$ elements to the right location?\nLet’s take a look at the dot products for $c_{1,1}$, $c_{1,2}$, and $c_{2,1}$. Visualize the propagation of the $c$ recurrence along the $k$-dimension above the point $i = 1, j = 1, k = 0$. Let’s position the row of $A$, $a_1^T$, alongside the $c_{1,1}$ propagation in the $j = 0$ plane. Thus, $a_{1,1}$ is presented at the lattice point $(1,0,1)$, and $a_{1,N}$ is positioned at the lattice point $(1,0,N)$. Similarly, let’s position the column of $B$, $b_1$, alongside the $c_{1,1}$ propagation, but position it in the $i = 0$ plane. That would imply that $b_{1,1}$ is presented at the lattice point $(0,1,1)$, and $b_{N,1}$ is positioned at the lattice point $(0,1,N)$. This would transform the recurrence equation for $c_{1,1}$ into:\n$$c_{1,1,k} = c_{1,1,k-1} + a_{1,0,k} * b_{0,1,k}$$ This recurrence represents all local neighborhood operand communications.\nIf we now visualize the recurrence for $c_{1,2}$ to propagate in the $k$-column above the lattice point $(1,2,0)$ we recognize that for $c_{1,2}$ we need the same row vector of $A$ as the recurrence for $c_{1,1}$. We can thus propagate the elements of the row vector $a_1^T$ along the $j$-direction and build up one side of the dot products for the row $c_1^T$.\nGeneralized to the whole $A$ matrix, this is a set of propagation recurrences defined by: $$ a_{i,j,k} = a_{i,j-1,k}$$\nSimilarly, the column vector $b_1^T$ is shared between the recurrence of $c_{1,1}$ and $c_{2,1}$, and we can propagate the elements of the column vector $b_1^T$ along the $i$-direction and build up the other side of the dot products for the column $c_1$. Generalized to the whole $B$ matrix, this is a set of propagation recurrences defined by: $$ b_{i,j,k} = b_{i-1,j,k}$$\nOnce we have the $A$ and $B$ matrix elements distributed throughout the lattice, we can finally transform the $c$ recurrence into a local neighborhood operand communication as well:\n$$c_{i,j,k} = c_{i,j,k-1} + a_{i,j-1,k} * b_{i-1,j,k}$$ This completes the transformation to all local neighborhood operand communications with the system of recurrences we have seen before expressed as a domain flow program:\ncompute ( (i,j,k) | 1 \u003c= i,j,k \u003c= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } 1: History of Matrices and Determinants\n2: Matrix Computations, Gene Golub and Charles van Loan\n","description":"","tags":["domain-flow","matrix-multiply","derivation"],"title":"Derivation","uri":"/introduction/derivation/index.html"},{"content":"Lorem Ipsum\n","description":"","tags":null,"title":"Gaussian Elimination","uri":"/linearsolvers/lu/index.html"},{"content":"Next Steps Now that we have a rudimentary understanding of parallel algorithms and their physical execution, the next step is to learn about what makes for a good/efficient parallel algorithm.\n","description":"","tags":null,"title":"Next Steps","uri":"/introduction/nextsteps/index.html"},{"content":"BLAS Level 3 are $\\mathcal{O}(N^3)$ operators, and finally compute bound creating many opportunities to optimize operand resuse.\nIn addition to matrix-matrix multiply there are the Rank-k update operators, which are outer products and matrix additions. Here is a Hermitian Rank-k update:\n$$ C = \\alpha A A^T + \\beta C, \\space where \\space C \\space is \\space Hermitian. $$\nA Hermitian matrix is defined as a matrix that is equal to its Hermitian conjugate. In other words, the matrix C is Hermitian if and only if $C = C^H$. Obviously a Hermitian matrix must be square. Hermitian matrices can be understood as the complex extension of real symmetric matrices.\nMatrix-matrix multiply We have seen this algorithm in the introduction:\ncompute ( (i,j,k) | 1 \u003c= i,j,k \u003c= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } Hermitian Rank-k update Starting from a single matrix, generating the transpose is a very expensive operator. For simplicity, let’s start with the computational phase $\\alpha A A^T$.\ncompute ( (i,j,k) | 1 \u003c= i,j,k \u003c= N ) { a: a[i,j-1,k] at: at[i-1,j,k] if k = 1 { c: beta[i,j,k-1] * c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } else { c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } } Here we introduce a conditional constraint that impacts the domain of computation for a set of equations.\n","description":"","tags":null,"title":"BLAS Level 3","uri":"/blas/level3/index.html"},{"content":"Lorem Ipsum.\n","description":"","tags":null,"title":"Webgl","uri":"/introduction/webgl/index.html"},{"content":"This is the quadratic equation:\n$$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$ ","description":"","tags":null,"title":"Matrix Factorizations","uri":"/factorization/factorization/index.html"},{"content":"Chapter 2 Elements of Good Design For sixty years, we have been optimizing for sequential computation. The best algorithms for sequential execution are those that minimize the number of operations to yield results. Computational complexity theory is well-aligned with this quest, but any performance-minded algorithm designer knows that the best theoretical algorithms are not necessarily the fastest when executed on real hardware. The difference is typically caused by the trade-off sequential algorithms have to make between computation and accessing memory.\nThis chapter explores the elements of good design for parallel algorithms and their execution on real hardware.\n","description":"","tags":null,"title":"Elements of Good Design","uri":"/design/index.html"},{"content":"Lorem Ipsum\n","description":"","tags":null,"title":"Matrix Kernels","uri":"/matrixkernels/matrixkernels/index.html"},{"content":"Chapter 3 Linear Algebra: the basics Basic Linear Algebra Subroutines are an historically significant set of functions that encapsulate the basic building blocks of a large collection of linear algebra algorithms and implementation.\nThe BLAS library has proven to be a very productive mechanism to create and disseminate highly optimized numerical libraries to a plethora of computer architectures and machines. Writing high-performance linear algebra algorithms turns out to be a tenacious problem, but since linear algebra operations are essential\ncomponents in computational methods, the investment can pay high dividends.\n","description":"","tags":null,"title":"Basic Linear Algebra","uri":"/blas/index.html"},{"content":"Chapter 4 Matrix Factorization Matrix factorizations are the work horse of linear algebra applications. Factorizations create equivalences that improve the usability or robustness of an algorithm.\n","description":"","tags":null,"title":"Matrix Factorization","uri":"/factorization/index.html"},{"content":"Chapter 5 Matrix Kernels Matrix Kernels are important to characterize and classify the underlying system of equations. Identifying singularity, and quantifying the null-space of a matrix are key operators before we can try to solve systems of equations.\n","description":"","tags":null,"title":"Matrix Kernels","uri":"/matrixkernels/index.html"},{"content":"Chapter 6 Linear Solvers Solving systems of equations is the impetus for the class of algorithms called linear solvers.\n","description":"","tags":null,"title":"Linear Solvers","uri":"/linearsolvers/index.html"},{"content":"","description":"","tags":null,"title":"Domain Flow Architecture","uri":"/index.html"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/index.html"},{"content":"","description":"","tags":null,"title":"derivation","uri":"/tags/derivation/index.html"},{"content":"","description":"","tags":null,"title":"design","uri":"/categories/design/index.html"},{"content":"","description":"","tags":null,"title":"domain-flow","uri":"/tags/domain-flow/index.html"},{"content":"","description":"","tags":null,"title":"domain-flow","uri":"/categories/domain-flow/index.html"},{"content":"","description":"","tags":null,"title":"free-schedule","uri":"/tags/free-schedule/index.html"},{"content":"","description":"","tags":null,"title":"linear-schedule","uri":"/tags/linear-schedule/index.html"},{"content":"","description":"","tags":null,"title":"matrix-math","uri":"/categories/matrix-math/index.html"},{"content":"","description":"","tags":null,"title":"matrix-multiply","uri":"/tags/matrix-multiply/index.html"},{"content":"","description":"","tags":null,"title":"schedule","uri":"/categories/schedule/index.html"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/index.html"},{"content":"","description":"","tags":null,"title":"algorithm","uri":"/tags/algorithm/index.html"},{"content":"","description":"","tags":null,"title":"introduction","uri":"/categories/introduction/index.html"},{"content":"","description":"","tags":null,"title":"computational-spacetime","uri":"/tags/computational-spacetime/index.html"},{"content":"","description":"","tags":null,"title":"index-space","uri":"/tags/index-space/index.html"},{"content":"","description":"","tags":null,"title":"lattice","uri":"/tags/lattice/index.html"},{"content":"","description":"","tags":null,"title":"spacetime","uri":"/categories/spacetime/index.html"}]