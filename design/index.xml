<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elements of Good Design on Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/design/index.html</link><description>Recent content in Elements of Good Design on Domain Flow Architecture</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2017 07:42:59 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/design/index.xml" rel="self" type="application/rss+xml"/><item><title>Computational Dynamics</title><link>https://stillwater-sc.github.io/domain-flow/design/currentstate/index.html</link><pubDate>Fri, 17 Feb 2017 08:59:30 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/currentstate/index.html</guid><description>A memory access in a physical machine can be very complex. For example, when a program accesses an operand located at an address that is not in physical memory, the processor registers a page miss. The performance difference between an access from the local L1 cache versus a page miss can be 8 orders of magnitude, that is, ten million times slower. An L1 cache hit is serviced in the order of 500pico-seconds, whereas a page miss can be as slow as 15 milli-seconds if it has to come from rotational storage.</description></item><item><title>Elements of Design</title><link>https://stillwater-sc.github.io/domain-flow/design/elements/index.html</link><pubDate>Wed, 15 Feb 2017 07:43:18 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/elements/index.html</guid><description>We can summarize the attributes of good parallel algorithm design as
low operation count, where operation count is defined as the sum of operators and operand accesses minimal operand movement minimal resource contention Item #1 is well-known by theoretical computer scientists.
Item #2 is well-known among high-performance algorithm designers.
Item #3 is well-known among hardware designers and computer engineers.
When designing domain flow algorithms, we are looking for an energy efficient embedding of a computational graph in space, and it is thus to be expected that we need to combine all three attributes of minimizing operator count, operand movement, and resource contention.</description></item><item><title>Data Flow Machine</title><link>https://stillwater-sc.github.io/domain-flow/design/dfm/index.html</link><pubDate>Fri, 17 Feb 2017 09:20:57 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/dfm/index.html</guid><description>In the late 60&amp;rsquo;s and 70&amp;rsquo;s when computer scientists were exploring parallel computation by building the first parallel machines and developing the parallel algorithm complexity theory, folk realized that this over-constrained specification was a real problem for concurrency. One proposal to rectify this was a natively parallel execution model called the Data Flow Machine (DFM). A Data Flow Machine uses a different resource contention management mechanism:
write an operand into an appropriate operand slot in an instruction token stored in a Content Addressable Memory (CAM) by an instruction tag check if all operands are present to start the execution cycle of the instruction if an instruction is ready then extract it from the CAM and inject it into a fabric of computational elements deliver the instruction to an available execution unit execute the instruction, and finally write the result back into an operand slot in target instruction token stored in the CAM The strength of the resource contention management of the Data Flow Machine is that the machine can execute along the free schedule, that is, the inherent parallelism of the algorithm.</description></item><item><title>Time: the when</title><link>https://stillwater-sc.github.io/domain-flow/design/time/index.html</link><pubDate>Wed, 15 Feb 2017 07:48:27 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/time/index.html</guid><description>Time</description></item><item><title>Space: the where</title><link>https://stillwater-sc.github.io/domain-flow/design/space/index.html</link><pubDate>Wed, 15 Feb 2017 07:49:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/space/index.html</guid><description>Space</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/design/nextsteps/index.html</link><pubDate>Wed, 15 Feb 2017 07:49:53 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/nextsteps/index.html</guid><description>In this short introduction to parallel algorithms in general and domain flow in particular, our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.
Once we get a good collection of fast, and energy efficient algorithms together, we can start to explore how best to engineer combinations of these operators. We will discover that sometimes, the cost of an information exchange makes a whole class of algorithms unattractive for parallel executions.</description></item></channel></rss>