<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elements of Good Design - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/design/index.html</link><description>The best algorithms for sequential execution are those that minimize the number of operations to yield results. Computational complexity theory has aided this quest, but any performance-minded algorithm designer knows that the best theoretical algorithms are not necessarily the fastest when executed on real hardware. The difference is typically caused by the trade-off sequential algorithms have to make between computation and accessing memory. The constraints of data movement are even more pronounced in parallel algorithms as demonstrated in the previous section.</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 17 Feb 2017 09:20:57 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/design/index.xml" rel="self" type="application/rss+xml"/><item><title>Computational Dynamics</title><link>https://stillwater-sc.github.io/domain-flow/design/currentstate/index.html</link><pubDate>Fri, 17 Feb 2017 08:59:30 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/currentstate/index.html</guid><description>A memory access in a physical machine can be very complex. For example, when a program accesses an operand located at an address that is not in physical memory, the processor registers a page miss. The performance difference between an access from the local L1 cache versus a page miss can be 8 orders of magnitude, that is, ten million times slower. An L1 cache hit is serviced in the order of 500pico-seconds, whereas a page miss can be as slow as 15 milli-seconds if it has to come from rotational storage.</description></item><item><title>Elements of Design</title><link>https://stillwater-sc.github.io/domain-flow/design/elements/index.html</link><pubDate>Wed, 15 Feb 2017 07:43:18 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/elements/index.html</guid><description>We can summarize the attributes of good parallel algorithm design as
low operation count, where operation count is defined as the sum of operators and operand accesses minimal operand movement minimal resource contention Item #1 is well-known by theoretical computer scientists.
Item #2 is well-known among high-performance algorithm designers.
Item #3 is well-known among hardware designers and computer engineers.
When designing domain flow algorithms, we are looking for an energy efficient embedding of a computational graph in space, and it is thus to be expected that we need to combine all three attributes of minimizing operator count, operand movement, and resource contention. The complexity of minimizing resource contention is what makes hardware design so much more complex. But the complexity of operator contention can be mitigated by clever resource contention management.</description></item><item><title>Data Flow Machine</title><link>https://stillwater-sc.github.io/domain-flow/design/dfm/index.html</link><pubDate>Fri, 17 Feb 2017 09:20:57 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/dfm/index.html</guid><description>In the late 60’s and 70’s when computer scientists were exploring parallel computation by building the first parallel machines and developing the parallel algorithm complexity theory, folk realized that this over-constrained specification was a real problem for concurrency. One proposal to rectify this was a natively parallel execution model called the Data Flow Machine (DFM). A Data Flow Machine uses a different resource contention management mechanism:
write an operand into an appropriate operand slot in an instruction token stored in a Content Addressable Memory (CAM) by an instruction tag check if all operands are present to start the execution cycle of the instruction if an instruction is ready then extract it from the CAM and inject it into a fabric of computational elements deliver the instruction to an available execution unit execute the instruction, and finally write the result back into an operand slot in target instruction token stored in the CAM The strength of the resource contention management of the Data Flow Machine is that the machine can execute along the free schedule, that is, the inherent parallelism of the algorithm. Any physical implementation, however, is constrained by the energy-efficiency of the CAM and the network that connects the CAM to the fabric of computational elements. As concurrency demands grow the efficiency of both the CAM and the fabric decreases making large data flow machines unattractive. However, small data flow machines don’t have this problem and are able to deliver energy-efficient, low-latency resource management. Today, all high-performance microprocessors have a data flow machine at their core.</description></item><item><title>Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/design/dfa/index.html</link><pubDate>Fri, 17 Feb 2017 09:20:57 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/dfa/index.html</guid><description>Domain Flow Architecture (DFA) machines are the class of machines that execute using the domain flow execution model. The fundamental problem limiting the energy efficiency of the data flow machine is the size of the CAM and fabric. As they are managed as two separate clusters of resources, they grow together. The domain flow execution model recognizes that for an important class of algorithms, we can distribute the CAM across the computational elements in the fabric, and we can scale the machine without negatively impacting the cycle time of execution.</description></item><item><title>Time: the when</title><link>https://stillwater-sc.github.io/domain-flow/design/time/index.html</link><pubDate>Wed, 15 Feb 2017 07:48:27 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/time/index.html</guid><description>The free schedule represents the inherent concurrency of the parallel algorithm, and, under the assumption of infinite resources, it is the fastest schedule possible.
Let x be a computation that uses y as input, then the free schedule is defined as: \begin{equation} T(x) =\begin{cases} 1, &amp; \text{if y is an external input}\\ 1 + max(T(y)) &amp; \text{otherwise} \end{cases} \end{equation} The free schedule is defined at the level of the individual operations. It does not provide any information about the global data movement or the global structure of the interactions between data and operation. Moreover, the above equation describes a logical sequencing of operations, it does not specify a physical evolution.</description></item><item><title>Space: the where</title><link>https://stillwater-sc.github.io/domain-flow/design/space/index.html</link><pubDate>Wed, 15 Feb 2017 07:49:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/space/index.html</guid><description>Space is a scarce resource, with a direct cost associated to it. A computational engine, such as a Stored Program Machine, needs to allocate area for ALUs and register files, and to make these work well, even more space is required to surround these resources with cache hierarchies and memory controllers. But even if space was freely available, it still presents a cost from a parallel computational perspective, since it takes energy to get information across space, as it takes time to do so.</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/design/nextsteps/index.html</link><pubDate>Wed, 15 Feb 2017 07:49:53 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/design/nextsteps/index.html</guid><description>In this short introduction to parallel algorithms in general and domain flow in particular, our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.
Once we get a good collection of fast, and energy efficient algorithms together, we can start to explore how best to engineer combinations of these operators. We will discover that sometimes, the cost of an information exchange makes a whole class of algorithms unattractive for parallel executions. With that insight comes the need to create new algorithms and sometimes completely new mathematical approaches to properly leverage the available resources.</description></item></channel></rss>