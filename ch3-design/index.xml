<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elements of Good Design - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/index.html</link><description>The best algorithms for sequential execution are those that minimize the number of operations to yield results. Computational complexity theory has aided this quest, but any performance-minded algorithm designer knows that the best theoretical algorithms are not necessarily the fastest when executed on real hardware. The difference is typically caused by the trade-off sequential algorithms have to make between computation and accessing memory. The constraints of data movement are even more pronounced in parallel algorithms as demonstrated in the previous section.</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 07 Jan 2025 15:31:27 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/ch3-design/index.xml" rel="self" type="application/rss+xml"/><item><title>Computational Dynamics</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/currentstate/index.html</link><pubDate>Fri, 17 Feb 2017 08:59:30 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/currentstate/index.html</guid><description>A memory access in a physical machine can be very complex. For example, when a program accesses an operand located at an address that is not in physical memory, the processor registers a page miss. The performance difference between an access from the local L1 cache versus a page miss can be 8 orders of magnitude, that is, ten million times slower. An L1 cache hit is serviced in the order of 500pico-seconds, whereas a page miss can be as slow as 15 milli-seconds if it has to come from rotational storage.</description></item><item><title>Elements of Design</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/elements/index.html</link><pubDate>Wed, 15 Feb 2017 07:43:18 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/elements/index.html</guid><description>We can summarize the attributes of good parallel algorithm design as
low operation count, where operation count is defined as the sum of operators and operand accesses minimal operand movement minimal resource contention Item #1 is well-known by theoretical computer scientists.
Item #2 is well-known among high-performance algorithm designers.
Item #3 is well-known among hardware designers and computer engineers.
When designing domain flow algorithms, we are looking for an energy efficient embedding of a computational graph in space, and it is thus to be expected that we need to combine all three attributes of minimizing operator count, operand movement, and resource contention. The complexity of minimizing resource contention is what makes hardware design so much more complex. But the complexity of operator contention can be mitigated by clever resource contention management.</description></item><item><title>Time: the when</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/time/index.html</link><pubDate>Wed, 15 Feb 2017 07:48:27 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/time/index.html</guid><description>The free schedule represents the inherent concurrency of the parallel algorithm, and, under the assumption of infinite resources, it is the fastest schedule possible.
Let x be a computation that uses y as input, then the free schedule is defined as: \begin{equation} T(x) =\begin{cases} 1, &amp; \text{if y is an external input}\\ 1 + max(T(y)) &amp; \text{otherwise} \end{cases} \end{equation} The free schedule is defined at the level of the individual operations. It does not provide any information about the global data movement or the global structure of the interactions between data and operation. Moreover, the above equation describes a logical sequencing of operations, it does not specify a physical evolution.</description></item><item><title>Space: the where</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/space/index.html</link><pubDate>Sun, 05 Jan 2025 13:39:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/space/index.html</guid><description>Space is a scarce resource, with a direct cost associated to it. A computational engine, such as a Stored Program Machine, needs to allocate area for ALUs and register files, and to make these work well, even more space is required to surround these resources with cache hierarchies and memory controllers. But even if space was freely available, it still presents a cost from a parallel computational perspective, since it takes energy to get information across space, as it takes time to do so.</description></item><item><title>Control: the how</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/control/index.html</link><pubDate>Tue, 07 Jan 2025 15:31:27 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/control/index.html</guid><description>Fundamentally, the Stored Program Machine (SPM) relies on a request/reply protocol to get information to and from memory. Otherwise stated, the resource contention mechanism deployed by a SPM uses a random access memory to store inputs, intermediate, and output values.
We have seen the Data Flow Machine (DFM) use a different mechanism. Here instructions fire when all their input data is available. When they fire, an instruction token is injected into a network of processing units to be executed. The result of that execution is encapsulated into a data token, which is send back to the central Content Addressable Memory where the token is matched with all the instructions it is part of.</description></item><item><title>Energy: the how efficient</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/energy/index.html</link><pubDate>Sun, 05 Jan 2025 13:39:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/energy/index.html</guid><description>Table 1 shows switching energy estimates of key computational events by process node. Data movement operations (reads and writes) have started to dominate energy consumption in modern processors. This makes a Stored Program Machine (SPM) less and less efficient. To counter this, all CPUs, GPUs, and DSPs have started to add instructions that amortize instruction processing among more computational intensity: they have all become SIMD machines.
Fundamentally, the SPM relies on a request/reply protocol to get information from a memory. Otherwise stated, the resource contention mechanism deployed by a SPM uses a random access memory to store inputs, intermediate, and output values. And all this memory management uses this request/reply cycle. Which we now know is becoming less and less energy efficient compared to the actual computational event the algorithm requires. The sequential processing model is becoming less and less energy efficient.</description></item><item><title>Next Steps</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/nextsteps/index.html</link><pubDate>Wed, 15 Feb 2017 07:49:53 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/nextsteps/index.html</guid><description>In this short introduction to parallel algorithms in general and domain flow in particular, our next step is to look at specific algorithms, and explore their optimal parallel execution dynamics.
Once we get a good collection of fast, and energy efficient algorithms together, we can start to explore how best to engineer combinations of these operators. We will discover that sometimes, the cost of an information exchange makes a whole class of algorithms unattractive for parallel executions. With that insight comes the need to create new algorithms and sometimes completely new mathematical approaches to properly leverage the available resources.</description></item><item><title>Switching Energy Estimates</title><link>https://stillwater-sc.github.io/domain-flow/ch3-design/switching-energy/index.html</link><pubDate>Sun, 05 Jan 2025 13:39:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch3-design/switching-energy/index.html</guid><description>This page contains background information regarding the switching energy estimates so important to designing energy-efficient data paths.
Register Read/Write Energy Estimates by Process Node Note: Values are approximate and may vary by foundry and implementation
Register 28/22nm (fJ) 16/14/12nm (fJ) 7/6/5nm (fJ) 3nm (fJ) 2nm (fJ) Read bit 2.5 - 3.5 1.8 - 2.3 0.9 - 1.2 0.6 - 0.8 0.4 - 0.6 Write bit 3.0 - 4.0 2.0 - 2.8 1.1 - 1.5 0.7 - 1.0 0.5 - 0.8 Notes:</description></item></channel></rss>