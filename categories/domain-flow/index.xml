<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>domain-flow on Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/categories/domain-flow/index.html</link><description>Recent content in domain-flow on Domain Flow Architecture</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2017 07:24:38 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/categories/domain-flow/index.xml" rel="self" type="application/rss+xml"/><item><title>An Example</title><link>https://stillwater-sc.github.io/domain-flow/introduction/example/index.html</link><pubDate>Wed, 15 Feb 2017 07:00:21 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/example/index.html</guid><description>Let&amp;rsquo;s look at a simple, but frequently used operator in Deep Learning inference: dense matrix multiplication. A Domain Flow program for this operator is shown below:
compute ( (i,j,k) | 1 &amp;lt;= i,j,k &amp;lt;= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } The underlying algorithm requires a domain of computation governed by a set of constraints, and a set of computational dependencies that implicitly define a partial order across all the operations in the computation.</description></item><item><title>Parallel Programming</title><link>https://stillwater-sc.github.io/domain-flow/introduction/parallel-programming/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/parallel-programming/index.html</guid><description>To appreciate the domain flow programming model and what it enables, you need to think about the physical form a &amp;lsquo;program evaluator&amp;rsquo; could take. In the days when a processor occupied the volume of a small room, any physical computational machine was limited to a single computational element. This implied that the execution of any algorithm had to be specified as a complete order in time. At each step of the execution, the computational element would read input operands, execute an instruction, and write a result.</description></item><item><title>Computational Spacetime</title><link>https://stillwater-sc.github.io/domain-flow/introduction/computational-spacetime/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/computational-spacetime/index.html</guid><description>Data movement in a parallel machine is constrained by propagation delay. The propagation delay in a communication channel acts similarly as the constraint on the speed of light in spacetime. Think back to the model of a physical parallel machine being a fixed graph in space with communication channels connecting computational nodes. The channels of communication constrain the propagation directions for information exchange. To enable fine-grain parallelism assume that computation and communication are separate steps in a computational even.</description></item><item><title>Domain Flow</title><link>https://stillwater-sc.github.io/domain-flow/introduction/domain-flow/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/domain-flow/index.html</guid><description>Domain flow is an abstract parallel programming model that is invariant to technology changes.
An equation $c = a \oplus b$ is comprised of a computation phase, the $\oplus$, and a communication phase, the $=$.
Implementation technology will impact these phases differently, and we are seeking a programming model that is invariant to the difference. A thought experiment will shed light on the desired properties of such a model.
In the extreme, if the two delays are very different, then the physical execution will either be computation-bound, or communication-bound.</description></item><item><title>Free Schedule</title><link>https://stillwater-sc.github.io/domain-flow/introduction/freeschedule/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/freeschedule/index.html</guid><description>We alluded to the fact that inherently-parallel algorithms exhibit some partial order, and not a total order, because the instructions that can execute independently do not have any explicit order among each other. This extra degree of freedom is another benefit domain flow algorithms exhibit over sequential algorithms. It allows the execution engine to organize any resource contention in a more energy, space, or time efficient way, as long the machine does not violate the dependency relationships specified in the algorithm.</description></item><item><title>Linear Schedules</title><link>https://stillwater-sc.github.io/domain-flow/introduction/linearschedule/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/linearschedule/index.html</guid><description>In the previous section, we saw what the computational evolution of an unconstrained parallel algorithm looks like. However, an actual physical system would have finite resources, and certainly limited operand bandwidth.
The free schedule of a parallel algorithm tends to be unrealizable as the size of the problem grows.
Let&amp;rsquo;s go through the thought experiment what the free schedule demands from a physical system. In the free schedule animation, the propagation recurrences distributing the $A$ and $B$ matrix elements throughout the 3D lattice run &amp;lsquo;ahead&amp;rsquo; of the actual computational recurrence calculating the $C$ matrix elements.</description></item><item><title>Derivation of the matrix multiply domain flow program</title><link>https://stillwater-sc.github.io/domain-flow/introduction/derivation/index.html</link><pubDate>Wed, 15 Feb 2017 07:24:38 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/introduction/derivation/index.html</guid><description>The concepts of partial and total orders are essential for finding optimal domain flow algorithms. Partial orders, or Poset, are the source of high-performance, low-power execution patterns.
The Linear Algebra universe is particularly rich in partial orders, something that has been exploited for centuries 1. Matrix Computations2 by Golub, and van Loan provide a comprehensive review. What follows may be a bit technical, but keep in mind the visualizations of the previous pages as you try to visualize what the math implies.</description></item></channel></rss>