<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction - Category - Domain Flow Architecture</title><link>https://stillwater-sc.github.io/domain-flow/categories/introduction/index.html</link><description/><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 15 Feb 2017 07:00:21 -0500</lastBuildDate><atom:link href="https://stillwater-sc.github.io/domain-flow/categories/introduction/index.xml" rel="self" type="application/rss+xml"/><item><title>An Example</title><link>https://stillwater-sc.github.io/domain-flow/ch1/example/index.html</link><pubDate>Wed, 15 Feb 2017 07:00:21 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch1/example/index.html</guid><description>Let’s look at a simple, but frequently used operator in Deep Learning inference: dense matrix multiplication. A Domain Flow program 1 for this operator is shown below:
compute ( (i,j,k) | 1 &lt;= i,j,k &lt;= N ) { a: a[i,j-1,k] b: b[i-1,j,k] c: c[i,j,k-1] + a[i,j-1,k] * b[i-1,j,k] } The underlying algorithm requires a domain of computation governed by a set of constraints, and a set of computational dependencies that implicitly define a partial order across all the operations in the computation. The partial order is readily visible in the need to have computed the result for $c[i,j,k-1]$ before the computation of $c[i,j,k]$ can commence. In contrast, the $a$ and $b$ recurrences are independent of each other.</description></item><item><title>Parallel Programming</title><link>https://stillwater-sc.github.io/domain-flow/ch1/parallel-programming/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch1/parallel-programming/index.html</guid><description>To appreciate the domain flow programming model and what it enables, we need to think about the physical form a ‘program evaluator’ could take. In the days when a processor occupied the volume of a small room, any physical computational machine was limited to a single computational element. This implied that the execution of any algorithm had to be specified as a complete order in time. At each step of the execution, the computational element would read input operands, execute an instruction, and write a result. The reading and writing of operands was from and to some storage mechanism.</description></item><item><title>Constraints of Spacetime</title><link>https://stillwater-sc.github.io/domain-flow/ch1/spacetime/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch1/spacetime/index.html</guid><description>If you visualize the ‘world’ from the perspective of an operand flowing through a machine, you realize that a physical machine creates a specific spatial constraint for the movement of data. Processing nodes are fixed in space, and information is exchanged between nodes to accomplish some transformation. Nodes consume and generate information, and communication links move information (program and data) between nodes. This leads to a simple abstraction of a physical parallel machine consisting of a fixed graph in space that constraints computation and communication events generated by the parallel algorithm.</description></item><item><title>Computational Spacetime</title><link>https://stillwater-sc.github.io/domain-flow/ch1/computational-spacetime/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch1/computational-spacetime/index.html</guid><description>Data movement in a parallel machine is constrained by propagation delay. The propagation delay in a communication channel acts similarly as the constraint on the speed of light in spacetime. Think back to the model of a physical parallel machine being a fixed graph in space with communication channels connecting computational nodes. The channels of communication constrain the propagation directions for information exchange. To enable fine-grain parallelism assume that computation and communication are separate steps in a computational event. By doing so, we have created an efficient pipeline that hides the latency of the communication phase. We can now leverage the mental model of spacetime to argue about partial orders of computational events that might be able to effect each other.</description></item><item><title>Domain Flow</title><link>https://stillwater-sc.github.io/domain-flow/ch1/domain-flow/index.html</link><pubDate>Wed, 15 Feb 2017 06:58:22 -0500</pubDate><guid>https://stillwater-sc.github.io/domain-flow/ch1/domain-flow/index.html</guid><description>Domain flow is an abstract parallel programming model that is invariant to technology changes.
An equation $c = a \oplus b$ is comprised of a computation phase, the $\oplus$, and a communication phase, the $=$.
Implementation technology will impact these phases differently, and we are seeking a programming model that is invariant to the difference. A thought experiment will shed light on the desired properties of such a model.</description></item></channel></rss>